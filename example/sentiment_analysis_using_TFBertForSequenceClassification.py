# -*- coding: utf-8 -*-
"""sentiment_analysis_直接使用分類class_預設只能用CLS token.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10_iImeSX8eFkg-YePiA8TcE3Q9fPcC_2
"""

!pip install transformers

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from transformers import (
    BertTokenizer,
    TFBertForSequenceClassification
)

tf.random.set_seed(42)

data_source = "https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv"
df = pd.read_csv(data_source, delimiter="\t", header=None)
df.rename(columns={0: "reviews", 1: "label"}, inplace=True)


smaller_batch = df[:2000]

label_series = smaller_batch["label"]

model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)

tokenized = smaller_batch["reviews"].apply(
    (lambda review: tokenizer.encode(review, add_special_tokens=True))
)


max_len = np.max([len(tokenized_review) for tokenized_review in tokenized])
min_len = np.min([len(tokenized_review) for tokenized_review in tokenized])
avg_len = np.mean([len(tokenized_review) for tokenized_review in tokenized])
print("The length of the longest review:", max_len)
print("The length of the shortest review:", min_len)
print("The average length of all the reviews:", avg_len)

shortest_reviews = [
    tokenized_review for tokenized_review in tokenized if len(tokenized_review) == 3
]
print(shortest_reviews, "\n")
for review in shortest_reviews:
    print(tokenizer.convert_ids_to_tokens(review, skip_special_tokens=False))

padded_ids = np.array([i + [0] * (max_len - len(i)) for i in tokenized.values])
attention_mask = np.where(padded_ids != 0, 1, 0)


label = label_series.to_numpy()
label = to_categorical(label, num_classes=2)

model = TFBertForSequenceClassification.from_pretrained(model_name)
model.summary()

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=["accuracy"],
)

inputs = {"input_ids":padded_ids, "attention_mask":attention_mask}
history = model.fit(inputs, label, batch_size=32, epochs=10, validation_split=0.2)

# %%
plt.figure(figsize=(8, 5))
plt.plot(history.history["accuracy"], label="acc")
plt.plot(history.history["val_accuracy"], label="val_acc")
plt.grid(True)
plt.xlabel("Epochs")
plt.ylabel("Acc")
plt.title("[CLS]")
plt.legend()
plt.show()